# Embedding Syntax and Semantics of Prepositions via Tensor Decomposition

**Authors:** Ivan Titov, et al.

## Abstract
Prepositions are function words that connect entities and events. We propose a novel tensor-based model that embeds prepositions by decomposing **word-triple counts**, enabling the model to capture their syntactic and semantic roles.

## Key Ideas
- Tensor decomposition of word triples (head, preposition, tail).
- Learn interpretable embeddings for prepositions.
- Improves downstream tasks such as parsing.

## Excerpt
> "Prepositions often encode fine-grained semantic relations, and our model explicitly represents them in a low-dimensional spaceâ€¦"
