# Word Embeddings via Tensor Factorization

**Authors:** Georgios Balikas, Tiberio Caetano, Massih-Reza Amini

## Abstract
We propose a new method for learning word embeddings by factorizing **higher-order co-occurrence tensors**. Unlike word2vec or GloVe, which rely on word-context co-occurrence matrices, this method captures multi-way interactions to represent word senses better.

## Key Ideas
- Factorize co-occurrence tensor (word, context1, context2, ...).
- Capture polysemy (multiple senses of words).
- Extend embeddings to richer linguistic structures.

## Excerpt
> "Tensor factorization allows embeddings to encode context-dependent meaning beyond pairwise co-occurrenceâ€¦"
